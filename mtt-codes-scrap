#!/usr/bin/env python
import requests
from bs4 import BeautifulSoup

TARGET_URL = 'https://www.mtt.ru/support/codes/'
DEFAULT_USERAGENT = (
    "Mozilla/5.0 (X11; Linux x86_64)"
    " AppleWebKit/537.36 (KHTML, like Gecko)"
    " Chrome/108.0.0.0 Safari/537.36"
)

import argparse
import csv
import logging
import sys
from typing import Iterator, Sequence

logger = logging.getLogger(__name__)


def fetch(session: requests.Session, page: int = 1) -> Iterator[list]:
    logger.debug(f"fetch {page=}")
    r = session.get(TARGET_URL, params={'page': f'page-{page}'})
    data = r.json()
    s = BeautifulSoup(data['html'], 'lxml')
    for tr in s.find_all('tr'):
        yield [td.text.strip() for td in tr.find_all('td')]


def main(argv: Sequence[str] = sys.argv[1:]) -> None:
    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
    p = argparse.ArgumentParser()
    p.add_argument(
        '-o',
        '--output',
        default='-',
        type=argparse.FileType('w+'),
    )
    p.add_argument('--user-agent', default=DEFAULT_USERAGENT)
    args = p.parse_args(argv)
    session = requests.session()
    session.headers.update(
        {
            'User-Agent': args.user_agent,
            'X-Requested-With': 'XMLHttpRequest',
        }
    )
    w = csv.writer(args.output)
    page = 1
    num_rows = 0
    while True:
        for row in fetch(session, page):
            w.writerow(row)
            num_rows += 1
        if num_rows % 20:
            break
        page += 1
    logger.debug("finished")


if __name__ == '__main__':
    main()
